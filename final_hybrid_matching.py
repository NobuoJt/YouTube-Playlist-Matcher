#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pandas as pd
import re
from difflib import SequenceMatcher
import unicodedata
import time

def normalize_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–"""
    if pd.isna(text):
        return ""
    text = unicodedata.normalize('NFKC', str(text)).lower()
    text = re.sub(r'[ï¼ˆï¼‰()]', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def extract_words(text):
    """åŠ¹ç‡çš„ãªå˜èªæŠ½å‡º"""
    if not text:
        return []
    
    text = normalize_text(text)
    
    # æ—¥æœ¬èªãƒ»è‹±èªå¢ƒç•Œã§åˆ†å‰²
    pattern = r'[\u4e00-\u9fff]+|[\u3040-\u309f]+|[\u30a0-\u30ff]+|[a-zA-Z0-9]+'
    words = re.findall(pattern, text)
    
    # è‹±èªã¯2æ–‡å­—ä»¥ä¸Šã€æ—¥æœ¬èªã¯1æ–‡å­—ä»¥ä¸Š
    result = []
    for word in words:
        if re.match(r'^[a-zA-Z0-9]+$', word):
            if len(word) >= 2:
                result.append(word)
        else:
            result.append(word)
    
    return result

def word_similarity(words1, words2):
    """å˜èªé¡ä¼¼åº¦è¨ˆç®—ï¼ˆé«˜é€Ÿç‰ˆï¼‰"""
    if not words1 or not words2:
        return 0.0, 0
    
    set1, set2 = set(words1), set(words2)
    common = len(set1 & set2)
    total = max(len(set1), len(set2))
    
    return common / total, common

def artist_similarity(artist1, artist2):
    """ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆé¡ä¼¼åº¦ï¼ˆé«˜é€Ÿç‰ˆï¼‰"""
    if not artist1 or not artist2:
        return 0.0
    
    a1 = normalize_text(artist1).replace(' - topic', '').replace(' official', '')
    a2 = normalize_text(artist2).replace(' - topic', '').replace(' official', '')
    
    if len(a1) < 2 or len(a2) < 2:
        return 0.0
    
    return SequenceMatcher(None, a1, a2).ratio()

def progress_bar(current, total, stage="", width=50):
    """é€²æ—ãƒãƒ¼ã®è¡¨ç¤º"""
    percent = current / total
    filled_width = int(width * percent)
    bar = 'â–ˆ' * filled_width + 'â–‘' * (width - filled_width)
    print(f'\r{stage} [{bar}] {current}/{total} ({percent*100:.1f}%)', end='', flush=True)

def main():
    print("ğŸµ æœ€çµ‚ç‰ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  v1.0")
    print("=" * 60)
    
    start_time = time.time()
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    try:
        original_df = pd.read_csv('list.csv', encoding='utf-8')
    except:
        original_df = pd.read_csv('list.csv', encoding='shift-jis')
    
    youtube_df = pd.read_csv('youtube_playlist.csv', encoding='utf-8')
    
    print(f"âœ… å…ƒãƒªã‚¹ãƒˆ: {len(original_df)}æ›²")
    print(f"âœ… YouTubeãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ: {len(youtube_df)}æ›²")
    
    # å‰å‡¦ç†ï¼šå˜èªåˆ†å‰²
    print("\nğŸ”§ å‰å‡¦ç†ä¸­ï¼ˆå˜èªåˆ†å‰²ï¼‰...")
    original_words = []
    for i, title in enumerate(original_df['.æ›²å']):
        progress_bar(i + 1, len(original_df), "å˜èªåˆ†å‰²")
        original_words.append(extract_words(title))
    
    print("\nğŸ”§ YouTubeã‚¿ã‚¤ãƒˆãƒ«å˜èªåˆ†å‰²ä¸­...")
    youtube_words = []
    for i, title in enumerate(youtube_df['æ›²å']):
        progress_bar(i + 1, len(youtube_df), "YTå˜èªåˆ†å‰²")
        youtube_words.append(extract_words(title))
    
    print("\n")
    
    matches = []
    used_youtube = set()
    
    # 3æ®µéšãƒãƒƒãƒãƒ³ã‚°
    thresholds = [
        (0.8, "ğŸ¯ é«˜ç²¾åº¦ãƒãƒƒãƒãƒ³ã‚°"),
        (0.6, "ğŸª ä¸­ç²¾åº¦ãƒãƒƒãƒãƒ³ã‚°"), 
        (0.4, "ğŸ” ä½ç²¾åº¦ãƒãƒƒãƒãƒ³ã‚°")
    ]
    
    total_processed = 0
    
    for threshold, stage_name in thresholds:
        print(f"\n{stage_name} (ã‚¹ã‚³ã‚¢ â‰¥ {threshold})")
        print("-" * 50)
        
        stage_count = 0
        unmatched_count = len(original_df) - len(matches)
        
        for i, row in original_df.iterrows():
            # æ—¢ã«ãƒãƒƒãƒæ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—
            if any(m['original_index'] == i + 1 for m in matches):
                continue
            
            # é€²æ—è¡¨ç¤º
            total_processed += 1
            if total_processed % 20 == 0 or total_processed <= 10:
                progress_bar(total_processed, unmatched_count, f"{stage_name}")
            
            best_score = 0.0
            best_match = None
            
            orig_words = original_words[i]
            orig_artist = row['.ä½œæ›²è€…']
            
            for j, yt_row in youtube_df.iterrows():
                if j in used_youtube:
                    continue
                
                yt_words = youtube_words[j]
                yt_artist = yt_row['ä½œæ›²è€…/ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ']
                
                # å˜èªé¡ä¼¼åº¦
                word_sim, common = word_similarity(orig_words, yt_words)
                
                # ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆé¡ä¼¼åº¦
                artist_sim = artist_similarity(orig_artist, yt_artist)
                
                # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢
                if artist_sim >= 0.8:
                    score = word_sim * 0.75 + artist_sim * 0.25
                    method = "hybrid_high_quality"
                elif artist_sim >= 0.5:
                    score = word_sim * 0.8 + artist_sim * 0.2
                    method = "hybrid_medium_quality"
                else:
                    score = word_sim * 0.9
                    method = "hybrid_word_based"
                
                if score >= threshold and score > best_score:
                    best_score = score
                    best_match = (j, yt_row, word_sim, artist_sim, common, method)
            
            if best_match:
                j, yt_row, word_sim, artist_sim, common, method = best_match
                used_youtube.add(j)
                stage_count += 1
                
                matches.append({
                    'original_index': i + 1,
                    'original_title': row['.æ›²å'],
                    'original_artist': row['.ä½œæ›²è€…'],
                    'youtube_index': j + 1,
                    'youtube_title': yt_row['æ›²å'],
                    'youtube_artist': yt_row['ä½œæ›²è€…/ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ'],
                    'youtube_url': yt_row['YouTubeURL'],
                    'video_id': yt_row['Video_ID'],
                    'hybrid_score': round(best_score, 3),
                    'word_similarity': round(word_sim, 3),
                    'artist_similarity': round(artist_sim, 3),
                    'common_words': common,
                    'method': method
                })
        
        print(f"\nâœ… {stage_count}æ›²ãƒãƒƒãƒãƒ³ã‚°å®Œäº†")
        total_processed = 0  # æ¬¡ã®ã‚¹ãƒ†ãƒ¼ã‚¸ç”¨ã«ãƒªã‚»ãƒƒãƒˆ
    
    # çµæœä¿å­˜
    print(f"\nğŸ’¾ çµæœã‚’ä¿å­˜ä¸­...")
    result_df = pd.DataFrame(matches)
    result_df.to_csv('final_hybrid_matches.csv', index=False, encoding='utf-8')
    
    # å‡¦ç†æ™‚é–“è¨ˆç®—
    elapsed_time = time.time() - start_time
    
    print(f"\nğŸ‰ ãƒãƒƒãƒãƒ³ã‚°å®Œäº†ï¼")
    print("=" * 60)
    print(f"ğŸ“Š ç·ãƒãƒƒãƒæ•°: {len(matches)}æ›²")
    print(f"ğŸ“ˆ ãƒãƒƒãƒç‡: {len(matches)/len(original_df)*100:.1f}%")
    print(f"â±ï¸ å‡¦ç†æ™‚é–“: {elapsed_time:.1f}ç§’")
    
    # å“è³ªçµ±è¨ˆ
    print(f"\nğŸ“‹ å“è³ªåˆ¥çµ±è¨ˆ:")
    method_counts = result_df['method'].value_counts()
    for method, count in method_counts.items():
        percentage = count / len(matches) * 100
        print(f"  â€¢ {method}: {count}æ›² ({percentage:.1f}%)")
    
    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    duplicates = result_df['video_id'].value_counts()
    dupe_count = (duplicates > 1).sum()
    print(f"\nğŸ”— Video IDé‡è¤‡: {dupe_count}ä»¶ {'âœ…' if dupe_count == 0 else 'âš ï¸'}")
    
    # é«˜å“è³ªãƒãƒƒãƒä¾‹
    high_quality = result_df[result_df['hybrid_score'] >= 0.95].head(5)
    if not high_quality.empty:
        print(f"\nâ­ å®Œç’§ãƒãƒƒãƒä¾‹ï¼ˆã‚¹ã‚³ã‚¢ â‰¥ 0.95ï¼‰:")
        for _, match in high_quality.iterrows():
            print(f"  â€¢ {match['original_title']} â†’ {match['youtube_title']}")
            print(f"    ã‚¹ã‚³ã‚¢: {match['hybrid_score']} | æ–¹å¼: {match['method']}")
    
    print(f"\nğŸ“ çµæœãƒ•ã‚¡ã‚¤ãƒ«: final_hybrid_matches.csv")
    print("ğŸµ ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")

if __name__ == "__main__":
    main()
